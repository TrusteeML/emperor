# Copyright 2015-2016 Stanford University
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http:#www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from .dt import *
from ..impl.simp import *
from ..util.log import *

from queue import PriorityQueue

# Flags for internal vs. leaf nodes
_DT_INTERNAL = 0
_DT_LEAF = 1

# Parameters for learning.
#
# fields:
#  tgtScore : float (target score at which to stop)
#  minGain : float (minimum gain to continue learning)
#  maxSize : int (maximum number of nodes to use)


class ParamsLearn:
    def __init__(self, tgtScore, minGain, maxSize):
        self.tgtScore = tgtScore
        self.minGain = minGain
        self.maxSize = maxSize

# Greedily learns a DT given a per-step learner "learnDTNode"
# and a sample distribution "dist". The parameter "initCons"
# represents the initial constraints to use for the root
# (e.g., empty).
#
# Essentially, we call learnDTNode(func, dist, cons), where
# the constraints are generated by the parent node, to construct
# the child node or leaf. Note that learnDTNode returns a branch
# (possibly None, if unsuccessful), which defines a function
# eval : X -> bool, left child constraints,
# right child constraints, and func : XS -> YS for a leaf node, and a gain.
#
# type parameters:
#  X = np.array([nCols])        : input domain
#  Y = np.float                 : output domain
#  XS = np.array([nPts, nCols]) : list of inputs
#  YS = np.array([nPts])        : list of outputs
#  C                            : metadata passed to child node (e.g., constraints for child)
#  D                            : sampling distribution
#
# parameters/returns:
#  learnDTNode : (XS -> YS) * (C * int -> [X]) * C ->
#                 (({eval : X -> bool} * C * C) | None) * float * {eval : X -> Y} * float
#  func        : XS -> YS
#  dist        : D
#  initCons    : C
#  params      : ParamsLearn
#  return      : DT


def learnDT(learnDTNode, func, dist, initCons, params):
    # Step 1: Initialize decision tree, score, and worklist

    # The decision tree is represented by dt, which is a map of type
    #
    #  type params:
    #    I : internal node
    #    L : leaf node
    #
    #  types:
    #    dt : {int : (I * _DT_INTERNAL) | (L * _DT_LEAF) }
    dt = {}
    worklist = PriorityQueue()
    index = 0
    depth = 1
    (dtInternalData, dtInternalScore, dtLeafData, dtLeafScore) = learnDTNode(func, dist, initCons)
    gain = dtInternalScore - dtLeafScore
    worklist.put_nowait((-gain, dtInternalData, dtLeafData, index, depth))
    score = dtLeafScore
    size = 1

    # Step 2: Iterate through the worklist and construct internal nodes
    while True:
        # Step 2a: Get the next element (break if worklist is empty)
        if worklist.empty():
            break
        (minusGain, dtInternalData, dtLeafData, index, depth) = worklist.get_nowait()
        gain = -minusGain
        log('Internal node index: ' + str(index), INFO)

        # Step 2b: Get the internal data, and add to decision tree
        (dtInternalNode, lcons, rcons) = dtInternalData
        if dtInternalNode.ind == None and dtInternalNode.thresh == None:
            log('No internal data!', INFO)
            worklist.put_nowait((2.0, dtInternalData, dtLeafData, index, depth))
            if gain < -1.5:
                log('No internal nodes remaining, ending!', INFO)
                break
            else:
                continue

        (dtInternalNode, lcons, rcons) = dtInternalData
        dt[index] = (dtInternalNode, _DT_INTERNAL)

        # Step 2c: Learn the left and right children
        (dtInternalDataLeft, dtInternalScoreLeft, dtLeafDataLeft, dtLeafScoreLeft) = learnDTNode(func, dist, lcons)
        (dtInternalDataRight, dtInternalScoreRight, dtLeafDataRight, dtLeafScoreRight) = learnDTNode(func, dist, rcons)
        gainLeft = dtInternalScoreLeft - dtLeafScoreLeft
        gainRight = dtInternalScoreRight - dtLeafScoreRight

        # Step 2d: Add children to worklist
        worklist.put_nowait((-gainLeft, dtInternalDataLeft, dtLeafDataLeft, 2 * index + 1, depth + 1))
        worklist.put_nowait((-gainRight, dtInternalDataRight, dtLeafDataRight, 2 * index + 2, depth + 1))

        # Step 2e: Compute score
        score += gain
        size += 2
        log('Current gain: ' + str(gain), INFO)
        log('Current score: ' + str(score), INFO)
        log('Current size: ' + str(size), INFO)

        # Step 2f: Check stopping conditions
        if not params.minGain is None and gain < params.minGain:
            log('Gain too small, ending!', INFO)
            break
        if not params.tgtScore is None and score >= params.tgtScore:
            log('Achieved target score, ending!', INFO)
            break
        if not params.maxSize is None and size >= params.maxSize:
            log('Reached maximum size, ending!', INFO)
            break
        if gain < -1.5:
            log('No internal nodes remaining, ending!', INFO)
            break

    # Step 3: Iterate through remaining nodes and construct leaf nodes
    while not worklist.empty():
        (minusGain, dtInternalData, dtLeafData, index, depth) = worklist.get_nowait()
        gain = -minusGain
        if dtInternalData is None and gain > 0.0:
            raise Exception('None node with non-zero gain: ' + str(dtInternalData))
        log('Leaf node index: ' + str(index), INFO)
        dt[index] = (dtLeafData, _DT_LEAF)

    # Step 4: Construct the decision tree
    return DT(_learnDTHelper(dt, 0))


def _learnDTHelper(dt, index):
    (data, nodeType) = dt[index]
    if nodeType == _DT_INTERNAL:
        return DTNode(data, _learnDTHelper(dt, 2 * index + 1), _learnDTHelper(dt, 2 * index + 2))
    elif nodeType == _DT_LEAF:
        return data
    else:
        raise Exception('Invalid node type: ' + str(nodeType))
